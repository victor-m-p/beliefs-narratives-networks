# OtreeAnalysis

Analysis pipeline for the beliefs-narratives-networks project. This code supports a preprint studying belief networks elicited through semi-structured interviews, LLM-assisted extraction, and participant curation across two longitudinal waves.

## Refactoring note

Between 2026-02-06 and 2026-02-08 the codebase was refactored with Claude Code to separate sensitive data from shareable analysis outputs. All scripts were verified, reproduced, and rerun by Victor Moller Poulsen. The pipeline produces identical results to the pre-refactor version.

## Directory structure

```
OtreeAnalysis/
  analysis/          # All scripts, run from this directory
  data/
    private/         # Sensitive data (gitignored): interviews, demographics, LLM prompts
    public/          # Sanitized data safe to share: edges, embeddings, topic models
  fig/               # All generated figures and tables (gitignored, regenerated by scripts)
```

## Data flow

Private raw data flows through three phases to produce public outputs and figures:

```
Phase 0 (sensitive)     Phase 1 (preprocessing)     Phase 2 (analysis + figures)
─────────────────────   ─────────────────────────   ────────────────────────────
00 pre_cleaning ──┐
01 curation ──────┤
02 post_cleaning ─┤
03 llm_nodes ─────┘──── 10 create_safe_json ──┐
                        11 distractors ───────┤
                        12 prep_data ─────────┘──── 20-35 analysis scripts ──── fig/
```

## Scripts

Scripts are numbered by phase. The first digit indicates the phase; scripts within a phase run in order.

### Phase 0: Raw data processing (requires private data)

| Script | Description |
|--------|-------------|
| `00_pre_cleaning_w1.py` | Cleans raw oTree CSV export for wave 1 (gitignored) |
| `00_pre_cleaning_w2.py` | Cleans raw oTree CSV export for wave 2 (gitignored) |
| `01_curation.py` | Parses cleaned CSV into structured JSON per participant |
| `02_post_cleaning.py` | Applies manual corrections to curated JSON |
| `03_llm_nodes.py` | Runs GPT-4.1 node and edge extraction on interview transcripts |

### Phase 1: Sanitization and preprocessing

| Script | Description |
|--------|-------------|
| `10_create_safe_json.py` | Strips sensitive fields (interviews, demographics, codes, LLM prompts) from participant JSON; produces public `curation_w*.json` and private transcripts |
| `11_distractors.py` | Filters to participants passing distractor attention checks; produces `distractors_w*.json` |
| `12_prep_data.py` | Extracts flat CSVs from filtered JSON: `edges_*.csv`, `interviews_w*.csv` |

### Phase 2: Analysis and figures

| Script | Output directory | Description |
|--------|-----------------|-------------|
| `20_pagetimes.py` | `fig/pagetimes/` | Survey completion time distributions |
| `21_rating_interviews.py` | `fig/ratings/` | Interview rating analysis |
| `22_rating_nodes.py` | `fig/ratings/` | Node (belief) rating distributions |
| `23_rating_networks.py` | `fig/rating_networks/` | Network-level rating comparisons |
| `24_training.py` | `fig/consistency/` | Training task accuracy (canvas interaction) |
| `25_consistency.py` | `fig/consistency/` | Cross-wave consistency of belief networks |
| `27_basic_reliability.py` | `fig/consistency/` | Test-retest reliability scatter panels |
| `28_bertopic_prepare.py` | `data/public/embeddings/` | Prepares node/edge tables and sentence embeddings for topic modelling |
| `29_bertopic_fit.py` | `data/public/bertopic/` | BERTopic grid search across embedding models and hyperparameters |
| `30_bertopic_select.py` | `data/public/bertopic/selection/` | Selects top-10 topic model fits by DBCV |
| `31_bertopic_retest.py` | `fig/BERTopic/retest/` | Test-retest reliability of topic assignments (phi coefficient, AUC) |
| `32_bertopic_map.py` | `data/public/bertopic_mapping/` | Maps topics to human edge and canvas data |
| `33_bertopic_plot.py` | `fig/BERTopic/stance_topic/` | Per-participant 2x2 stance/topic network plots |
| `34_bertopic_pred.py` | `fig/BERTopic/prediction/` | Topic-level predictive analysis |
| `35_bertopic_tables.py` | `fig/tables/` | LaTeX tables for the preprint (phi/AUC summary, topic overview) |

### Shared modules

| File | Used by |
|------|---------|
| `utilities.py` | All scripts (constants, path helpers, embedding model specs) |
| `helpers.py` | `01`, `23`, `28`, `29`, `31`, `34` (data extraction, embedding, plotting) |
| `llm_utilities.py` | `03` (OpenAI API calls, prompt templates, Pydantic response models) |

## Reproduction

All scripts are designed to be run from the `analysis/` directory. Phase 0 requires private data and API keys. Phases 1-2 can be rerun from the public sanitized JSON files.

```bash
cd OtreeAnalysis/analysis
python 20_pagetimes.py   # example: regenerates fig/pagetimes/
```

Phase 2 scripts (28-35) for topic modelling require `sentence-transformers`, `bertopic`, `umap-learn`, and `hdbscan`. The BERTopic grid search (`29`) takes approximately 30-60 minutes depending on hardware.
