"""
VMP 2026-02-06:

Creates sanitized versions of curation_w*.json files
that are safe to push to GitHub (no participant-sensitive information).

Removes:
- interviews (contains Q&A text)
- LLM prompts (contain full interview transcripts)
- LLM edge justifications (paraphrase interview content)
- interview_feedback, final_feedback (open text)
- demographics (entire field; not used in analysis)
- code (participant code; not used in analysis)

For pairwise_interview: keeps only safe fields (pair_index, pair, connection_choice).

Also generates:
- private/transcripts/transcripts_w*/  (full interview text files)

Note: interviews_w*.csv is generated by 12_prep_data.py from filtered data.
"""

import json
import os
import re
import copy
import textwrap
from utilities import wave_1, wave_2, get_private_path, get_public_path

# Configuration
WAVES = [wave_1, wave_2]

# Safe fields to keep from each pairwise_interview entry
PAIRWISE_SAFE_FIELDS = ['pair_index', 'pair', 'connection_choice']


def sanitize_participant(participant_data):
    """
    Remove sensitive fields from a single participant's data.
    Returns a deep copy with sensitive fields removed.
    """
    safe_data = copy.deepcopy(participant_data)

    # Extract interview metadata (word/char counts) before deleting text
    items = (safe_data.get("interviews", {}).get("main") or [])
    if items:
        safe_data["interview_metadata"] = [
            {
                "num": i,
                "mode": qa.get("input_mode"),
                "words_q": len((qa.get("question") or "").split()),
                "words_a": len((qa.get("answer") or "").split()),
                "char_q": len(qa.get("question") or ""),
                "char_a": len(qa.get("answer") or ""),
            }
            for i, qa in enumerate(items, 1)
        ]

    # Remove entire sensitive top-level fields
    sensitive_fields = [
        'interviews',           # Contains Q&A text
        'interview_feedback',   # Open text feedback
        'final_feedback',       # Open text feedback
        'demographics',         # Entire field; not used in analysis
        'code',                 # Participant code; not used
    ]

    for field in sensitive_fields:
        if field in safe_data:
            del safe_data[field]

    # Sanitize pairwise_interview: keep only safe fields
    if 'pairwise_interview' in safe_data and safe_data['pairwise_interview']:
        safe_data['pairwise_interview'] = [
            {k: entry[k] for k in PAIRWISE_SAFE_FIELDS if k in entry}
            for entry in safe_data['pairwise_interview']
        ]

    # Remove sensitive LLM fields (keep only results, not prompts)
    if 'LLM' in safe_data:
        if 'node_prompt' in safe_data['LLM']:
            del safe_data['LLM']['node_prompt']
        if 'edge_prompt' in safe_data['LLM']:
            del safe_data['LLM']['edge_prompt']

        # Remove justification field from edge_results (contains interview paraphrases)
        if 'edge_results' in safe_data['LLM'] and safe_data['LLM']['edge_results']:
            for edge in safe_data['LLM']['edge_results']:
                if 'justification' in edge:
                    del edge['justification']

    return safe_data


# ============================================================
# Transcript + interview metadata generation
# ============================================================

def wrap_block(text, label, width=88):
    indent_w = len(label)
    paras = re.split(r"\n\s*\n", (text or "").strip()) or [""]
    parts = []
    for i, p in enumerate(paras):
        parts.append(textwrap.fill(
            p,
            width=width,
            initial_indent=label if i == 0 else " " * indent_w,
            subsequent_indent=" " * indent_w,
            replace_whitespace=False,
        ))
    return "\n".join(parts)


def save_qa_txt(items, path, width=88):
    lines = []
    for idx, qa in enumerate(items, 1):
        q = (qa.get("question") or "").strip()
        a = (qa.get("answer") or "").strip()
        lines.append(wrap_block(q, f"Q{idx}: ", width))
        lines.append("")
        lines.append(wrap_block(a, f"A{idx}: ", width))
        lines.append("")
        lines.append("-" * min(80, width))
        lines.append("")
    with open(path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines).rstrip() + "\n")


def generate_transcripts(data, wave):
    """
    From private curation data, generate:
    - private/transcripts/transcripts_w{wave}/ (full Q&A text files)

    Note: interviews_w*.csv (word-count metadata) is now generated by
    12_prep_data.py from the filtered distractors data, using the
    interview_metadata field added during sanitization.
    """
    transcript_dir = get_private_path(f"transcripts/transcripts_w{wave}")
    os.makedirs(transcript_dir, exist_ok=True)

    for key, d in data.items():
        items = (d.get("interviews", {}).get("main") or [])
        if items:
            save_qa_txt(
                items,
                os.path.join(transcript_dir, f"{key}_transcript.txt"),
                width=88,
            )

    print(f"  Transcripts: {transcript_dir} ({len(data)} files)")


# ============================================================
# Main
# ============================================================

def main():
    print("=" * 60)
    print("Creating safe (sanitized) curation files")
    print("=" * 60)
    print()

    for wave in WAVES:
        print(f"--- Wave {wave} ---")

        curation_input = get_private_path("curation_w{wave}.json", wave=wave)
        curation_output = get_public_path("curation_w{wave}.json", wave=wave)

        if not os.path.exists(curation_input):
            print(f"  WARNING: {curation_input} not found, skipping")
            print()
            continue

        # Load private data
        print(f"  Loading: {curation_input}")
        with open(curation_input, 'r') as f:
            data = json.load(f)
        print(f"  Participants: {len(data)}")

        # Generate transcripts (private)
        generate_transcripts(data, wave)

        # Sanitize and save public curation file
        safe_data = {}
        for key, participant in data.items():
            safe_data[key] = sanitize_participant(participant)

        os.makedirs(os.path.dirname(curation_output), exist_ok=True)
        with open(curation_output, 'w') as f:
            json.dump(safe_data, f, indent=2)

        print(f"  Safe curation: {curation_output}")
        print()

if __name__ == "__main__":
    main()
