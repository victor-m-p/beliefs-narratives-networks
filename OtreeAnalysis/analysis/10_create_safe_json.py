"""
VMP 2026-02-06:

Creates sanitized versions of curation_w*.json files
that are safe to push to GitHub (no participant-sensitive information).

Removes:
- interviews (contains Q&A text)
- LLM prompts (contain full interview transcripts)
- LLM edge justifications (paraphrase interview content)
- interview_feedback, final_feedback (open text)
- demographics (entire field; not used in analysis)
- code (participant code; not used in analysis)
- network_compare[].reason (free-text participant commentary)
- LLM.edge_random[].justification (paraphrases interview content)

For pairwise_interview: keeps only safe fields (pair_index, pair, connection_choice).

Also:
- Replaces personal names in belief stances with anonymized placeholders
- Redacts justification fields from llm_extractions/ edge JSONs
- Generates private/transcripts/transcripts_w*/ (full interview text files)

Note: interviews_w*.csv is generated by 12_prep_data.py from filtered data.
"""

import json
import os
import re
import copy
import textwrap
from pathlib import Path
from utilities import wave_1, wave_2, get_private_path, get_public_path

# Configuration
WAVES = [wave_1, wave_2]

# Safe fields to keep from each pairwise_interview entry
PAIRWISE_SAFE_FIELDS = ['pair_index', 'pair', 'connection_choice']

# Name replacements: {original: replacement}
# Add entries here if new personal names are found in belief stances.
NAME_REPLACEMENTS = {
    "Erin": "A friend",
}


def replace_names(text):
    """Replace personal names in a string using NAME_REPLACEMENTS."""
    for name, replacement in NAME_REPLACEMENTS.items():
        text = text.replace(name, replacement)
    return text


def replace_names_recursive(obj):
    """Walk a JSON-like object and replace names in all string values."""
    if isinstance(obj, str):
        return replace_names(obj)
    elif isinstance(obj, list):
        return [replace_names_recursive(item) for item in obj]
    elif isinstance(obj, dict):
        return {k: replace_names_recursive(v) for k, v in obj.items()}
    return obj


def sanitize_participant(participant_data):
    """
    Remove sensitive fields from a single participant's data.
    Returns a deep copy with sensitive fields removed.
    """
    safe_data = copy.deepcopy(participant_data)

    # Extract interview metadata (word/char counts) before deleting text
    items = (safe_data.get("interviews", {}).get("main") or [])
    if items:
        safe_data["interview_metadata"] = [
            {
                "num": i,
                "mode": qa.get("input_mode"),
                "words_q": len((qa.get("question") or "").split()),
                "words_a": len((qa.get("answer") or "").split()),
                "char_q": len(qa.get("question") or ""),
                "char_a": len(qa.get("answer") or ""),
            }
            for i, qa in enumerate(items, 1)
        ]

    # Remove entire sensitive top-level fields
    sensitive_fields = [
        'interviews',           # Contains Q&A text
        'interview_feedback',   # Open text feedback
        'final_feedback',       # Open text feedback
        'demographics',         # Entire field; not used in analysis
        'code',                 # Participant code; not used
    ]

    for field in sensitive_fields:
        if field in safe_data:
            del safe_data[field]

    # Sanitize pairwise_interview: keep only safe fields
    if 'pairwise_interview' in safe_data and safe_data['pairwise_interview']:
        safe_data['pairwise_interview'] = [
            {k: entry[k] for k in PAIRWISE_SAFE_FIELDS if k in entry}
            for entry in safe_data['pairwise_interview']
        ]

    # Remove sensitive LLM fields (keep only results, not prompts)
    if 'LLM' in safe_data:
        if 'node_prompt' in safe_data['LLM']:
            del safe_data['LLM']['node_prompt']
        if 'edge_prompt' in safe_data['LLM']:
            del safe_data['LLM']['edge_prompt']

        # Remove justification field from edge_results (contains interview paraphrases)
        if 'edge_results' in safe_data['LLM'] and safe_data['LLM']['edge_results']:
            for edge in safe_data['LLM']['edge_results']:
                if 'justification' in edge:
                    del edge['justification']

        # Remove justification field from edge_random (also contains paraphrases)
        if 'edge_random' in safe_data['LLM'] and isinstance(safe_data['LLM']['edge_random'], list):
            for edge in safe_data['LLM']['edge_random']:
                if isinstance(edge, dict) and 'justification' in edge:
                    del edge['justification']

    # Remove free-text reason from network_compare entries
    for nc in (safe_data.get("network_compare") or []):
        if "reason" in nc:
            nc["reason"] = ""

    # Replace personal names throughout all remaining string fields
    safe_data = replace_names_recursive(safe_data)

    return safe_data


# ============================================================
# Transcript + interview metadata generation
# ============================================================

def wrap_block(text, label, width=88):
    indent_w = len(label)
    paras = re.split(r"\n\s*\n", (text or "").strip()) or [""]
    parts = []
    for i, p in enumerate(paras):
        parts.append(textwrap.fill(
            p,
            width=width,
            initial_indent=label if i == 0 else " " * indent_w,
            subsequent_indent=" " * indent_w,
            replace_whitespace=False,
        ))
    return "\n".join(parts)


def save_qa_txt(items, path, width=88):
    lines = []
    for idx, qa in enumerate(items, 1):
        q = (qa.get("question") or "").strip()
        a = (qa.get("answer") or "").strip()
        lines.append(wrap_block(q, f"Q{idx}: ", width))
        lines.append("")
        lines.append(wrap_block(a, f"A{idx}: ", width))
        lines.append("")
        lines.append("-" * min(80, width))
        lines.append("")
    with open(path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines).rstrip() + "\n")


def generate_transcripts(data, wave):
    """
    From private curation data, generate:
    - private/transcripts/transcripts_w{wave}/ (full Q&A text files)

    Note: interviews_w*.csv (word-count metadata) is now generated by
    12_prep_data.py from the filtered distractors data, using the
    interview_metadata field added during sanitization.
    """
    transcript_dir = get_private_path(f"transcripts/transcripts_w{wave}")
    os.makedirs(transcript_dir, exist_ok=True)

    for key, d in data.items():
        items = (d.get("interviews", {}).get("main") or [])
        if items:
            save_qa_txt(
                items,
                os.path.join(transcript_dir, f"{key}_transcript.txt"),
                width=88,
            )

    print(f"  Transcripts: {transcript_dir} ({len(data)} files)")


# ============================================================
# Redact llm_extractions/ edge JSONs
# ============================================================

def redact_llm_extractions():
    """
    Post-process llm_extractions/ edge files:
    - Remove justification fields from edge extraction JSONs
    - Replace personal names in all node and edge extraction JSONs
    """
    public_dir = Path(get_public_path("")).resolve()
    llm_dir = public_dir / "llm_extractions"
    if not llm_dir.exists():
        print("  llm_extractions/ not found, skipping")
        return

    edge_dirs = sorted(llm_dir.glob("edge_extraction_w*"))
    node_dirs = sorted(llm_dir.glob("node_extraction_w*"))

    # Edge extractions: remove justification + replace names
    n_just = 0
    for edge_dir in edge_dirs:
        for json_path in sorted(edge_dir.rglob("*.json")):
            with open(json_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            changed = False
            for edge in data:
                if "justification" in edge:
                    del edge["justification"]
                    n_just += 1
                    changed = True
            data = replace_names_recursive(data)
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
        print(f"  Redacted: {edge_dir.name}")

    # Node extractions: replace names only
    for node_dir in node_dirs:
        for json_path in sorted(node_dir.rglob("*.json")):
            with open(json_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            data = replace_names_recursive(data)
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
        print(f"  Redacted: {node_dir.name}")

    print(f"  Removed {n_just} justification fields from edge extractions")


# ============================================================
# Main
# ============================================================

def main():
    print("=" * 60)
    print("Creating safe (sanitized) curation files")
    print("=" * 60)
    print()

    for wave in WAVES:
        print(f"--- Wave {wave} ---")

        curation_input = get_private_path("curation_w{wave}.json", wave=wave)
        curation_output = get_public_path("curation_w{wave}.json", wave=wave)

        if not os.path.exists(curation_input):
            print(f"  WARNING: {curation_input} not found, skipping")
            print()
            continue

        # Load private data
        print(f"  Loading: {curation_input}")
        with open(curation_input, 'r') as f:
            data = json.load(f)
        print(f"  Participants: {len(data)}")

        # Generate transcripts (private)
        generate_transcripts(data, wave)

        # Sanitize and save public curation file
        safe_data = {}
        for key, participant in data.items():
            safe_data[key] = sanitize_participant(participant)

        os.makedirs(os.path.dirname(curation_output), exist_ok=True)
        with open(curation_output, 'w') as f:
            json.dump(safe_data, f, indent=2)

        print(f"  Safe curation: {curation_output}")
        print()

    # Redact llm_extractions/ (edge justifications + names)
    print("--- Redacting llm_extractions/ ---")
    redact_llm_extractions()
    print()

if __name__ == "__main__":
    main()
